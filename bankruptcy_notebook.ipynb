{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halimahbatam18/branch-AI/blob/main/bankruptcy_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6503ce25",
      "metadata": {
        "id": "6503ce25"
      },
      "source": [
        "\n",
        "# Bankruptcy Prediction — Notebook\n",
        "\n",
        "\n",
        "**Tujuan:** Membangun pipeline untuk memprediksi kebangkrutan bank sesuai arahan dosen.\n",
        "\n",
        "\n",
        "**Termasuk:** EDA (2+ visualisasi terhadap target), penanganan imbalance, modelling (Decision Tree, Random Forest, ANN), evaluasi lengkap (Confusion Matrix, Precision, Recall, F1), dan ekstraksi 5 fitur paling penting.\n",
        "\n",
        "Catatan: Jika ingin menggunakan file CSV asli, upload file dan ubah `DATA_PATH`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3bfab2",
      "metadata": {
        "id": "5b3bfab2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Imports ======\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "import joblib\n",
        "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
        "\n",
        "# ====== Data path ======\n",
        "DATA_PATH = None  # <-- ubah jika ada dataset: e.g. '/mnt/data/yourfile.csv'\n",
        "\n",
        "def make_synthetic_bankrupt_dataset(n_samples=2000, random_state=42):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    debt_ratio = rng.beta(2,5,size=n_samples) * 2.0\n",
        "    roa = rng.normal(0.02, 0.05, size=n_samples)\n",
        "    net_profit_margin = rng.normal(0.05, 0.07, size=n_samples)\n",
        "    liquidity_ratio = rng.normal(1.5, 0.7, size=n_samples)\n",
        "    op_exp_ratio = rng.normal(0.6, 0.2, size=n_samples)\n",
        "    asset_growth = rng.normal(0.03,0.1,size=n_samples)\n",
        "    risk_score = (debt_ratio*1.8) + (-roa*8.0) + (op_exp_ratio*1.5) + (-liquidity_ratio*0.8) + (asset_growth* -1.0)\n",
        "    prob = 1 / (1 + np.exp(- (risk_score - 0.5)))\n",
        "    threshold = np.percentile(prob, 88)\n",
        "    bankrupt = (prob > threshold).astype(int)\n",
        "    df = pd.DataFrame({\n",
        "        'debt_ratio': debt_ratio,\n",
        "        'roa': roa,\n",
        "        'net_profit_margin': net_profit_margin,\n",
        "        'liquidity_ratio': liquidity_ratio,\n",
        "        'op_exp_ratio': op_exp_ratio,\n",
        "        'asset_growth': asset_growth,\n",
        "        'bankrupt': bankrupt\n",
        "    })\n",
        "    return df\n",
        "\n",
        "# Load or create\n",
        "if DATA_PATH and os.path.exists(DATA_PATH):\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(\"Loaded:\", DATA_PATH)\n",
        "else:\n",
        "    df = make_synthetic_bankrupt_dataset(2000)\n",
        "    print(\"Using synthetic dataset\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263606fb",
      "metadata": {
        "id": "263606fb"
      },
      "source": [
        "## Exploratory Data Analysis (EDA) — minimal 2 visualisasi terhadap target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c112135",
      "metadata": {
        "id": "7c112135"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== EDA ======\n",
        "print(\"Class counts:\\n\", df['bankrupt'].value_counts())\n",
        "print(\"\\nClass percentages:\\n\", (df['bankrupt'].value_counts(normalize=True)*100).round(2))\n",
        "\n",
        "display(df.describe().T)\n",
        "\n",
        "# Visualisasi 1: Debt ratio distribution by target (boxplot)\n",
        "plt.figure()\n",
        "df.boxplot(column='debt_ratio', by='bankrupt')\n",
        "plt.title('Debt Ratio by Bankrupt'); plt.suptitle(''); plt.show()\n",
        "\n",
        "# Visualisasi 2: ROA distribution by target (boxplot)\n",
        "plt.figure()\n",
        "df.boxplot(column='roa', by='bankrupt')\n",
        "plt.title('ROA by Bankrupt'); plt.suptitle(''); plt.show()\n",
        "\n",
        "# Visualisasi 3: Scatter roa vs debt_ratio colored by target\n",
        "plt.figure()\n",
        "colors = df['bankrupt'].map({0:'C0',1:'C1'})\n",
        "plt.scatter(df['debt_ratio'], df['roa'], c=colors, alpha=0.6)\n",
        "plt.xlabel('debt_ratio'); plt.ylabel('roa'); plt.title('roa vs debt_ratio (by bankrupt)'); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7ad23f",
      "metadata": {
        "id": "5c7ad23f"
      },
      "source": [
        "## Data Preparation — cek imbalance & scaling (sesuai arahan dosen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e12482",
      "metadata": {
        "id": "04e12482"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Data Preparation ======\n",
        "# 1. Missing & duplicates\n",
        "print(\"Missing values per column:\\n\", df.isna().sum())\n",
        "before = df.shape[0]\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Dropped {before - df.shape[0]} duplicates\")\n",
        "\n",
        "# 2. Feature / label split\n",
        "X = df.drop(columns=['bankrupt'])\n",
        "y = df['bankrupt']\n",
        "\n",
        "# 3. Train-test split (stratify)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# 4. Check imbalance\n",
        "print(\"Train class distribution before handling:\", y_train.value_counts().to_dict())\n",
        "\n",
        "# ====== Option A: Random oversampling (implemented) ======\n",
        "train = pd.concat([X_train, y_train], axis=1)\n",
        "minority = train[train['bankrupt']==1]\n",
        "majority = train[train['bankrupt']==0]\n",
        "if len(minority)>0 and len(minority) < len(majority):\n",
        "    minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
        "    train_bal = pd.concat([majority, minority_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    X_train = train_bal.drop(columns=['bankrupt']); y_train = train_bal['bankrupt']\n",
        "    print(\"After random oversampling:\", y_train.value_counts().to_dict())\n",
        "else:\n",
        "    print(\"No oversampling needed\")\n",
        "\n",
        "# ====== Option B: SMOTE (commented) ======\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# sm = SMOTE(random_state=42)\n",
        "# X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "# print(\"After SMOTE:\", pd.Series(y_train_sm).value_counts())\n",
        "\n",
        "# 5. Scaling for ANN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "joblib.dump(scaler, '/mnt/data/scaler.joblib')\n",
        "print(\"Saved scaler to /mnt/data/scaler.joblib\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7886d04",
      "metadata": {
        "id": "e7886d04"
      },
      "source": [
        "## Modelling & Evaluation — Decision Tree, Random Forest, ANN; tampilkan Confusion Matrix, Precision, Recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de934960",
      "metadata": {
        "id": "de934960"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Helper: plot confusion matrix ======\n",
        "import seaborn as sns\n",
        "def plot_cm(cm, labels=[0,1], title='Confusion Matrix'):\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# ====== Train models ======\n",
        "models = {\n",
        "    'DecisionTree': DecisionTreeClassifier(max_depth=6, random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(64,32), max_iter=300, random_state=42)\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(\"\\n=== Training\", name, \"===\")\n",
        "    if name == 'MLP':\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "    trained[name] = model\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plot_cm(cm, labels=[0,1], title=f'Confusion Matrix - {name}')\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1]) if hasattr(model, \"predict_proba\") else None\n",
        "    except Exception:\n",
        "        auc = None\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    results[name] = {'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc}\n",
        "    print(f\"Metrics -> Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, AUC: {auc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94cf01e",
      "metadata": {
        "id": "b94cf01e"
      },
      "source": [
        "## Feature Importance — sebutkan top-5 variabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02bf0bc",
      "metadata": {
        "id": "b02bf0bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Feature importance and top-5 features ======\n",
        "rf = trained.get('RandomForest')\n",
        "if rf is not None:\n",
        "    feat_names = X.columns.tolist()\n",
        "    importances = rf.feature_importances_\n",
        "    fi = pd.DataFrame({'feature': feat_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
        "    display(fi)\n",
        "    print(\"\\nTop-5 features influencing bankruptcy:\")\n",
        "    print(fi.head(5).to_string(index=False))\n",
        "    # plot\n",
        "    plt.figure(); plt.barh(fi['feature'], fi['importance']); plt.gca().invert_yaxis(); plt.title('Feature Importance'); plt.xlabel('Importance'); plt.show()\n",
        "else:\n",
        "    print(\"RandomForest not trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0dda6e",
      "metadata": {
        "id": "cc0dda6e"
      },
      "source": [
        "## Save models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bfc915d",
      "metadata": {
        "id": "8bfc915d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Save models ======\n",
        "for name, model in trained.items():\n",
        "    path = f'/mnt/data/model_{name}.joblib'\n",
        "    joblib.dump(model, path)\n",
        "    print(\"Saved\", name, \"->\", path)\n",
        "print(\"\\nSelesai. Periksa /mnt/data untuk model dan scaler.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}